# LocalGPT: Sichere, lokale Gespr√§che mit Ihren Dokumenten üåê

**LocalGPT** ist eine Open-Source-Initiative, die es erm√∂glicht, mit Ihren Dokumenten zu kommunizieren, ohne Ihre Privatsph√§re zu gef√§hrden. Da alles lokal ausgef√ºhrt wird, k√∂nnen Sie sicher sein, dass keine Daten Ihren Computer verlassen. Tauchen Sie ein in die Welt sicherer, lokaler Dokumenteninteraktionen mit LocalGPT.

## Eigenschaften üåü
- **H√∂chste Privatsph√§re**: Ihre Daten bleiben auf Ihrem Computer und gew√§hrleisten 100% Sicherheit.
- **Vielseitige Modellunterst√ºtzung**: Integrieren Sie nahtlos eine Vielzahl von Open-Source-Modellen, einschlie√ülich HF, GPTQ, GGML und GGUF.
- **Vielf√§ltige Einbettungen**: W√§hlen Sie aus einer Reihe von Open-Source-Einbettungen.
- **Wiederverwendung Ihres LLM**: Nach dem Herunterladen k√∂nnen Sie Ihr LLM wieder verwenden, ohne es erneut herunterladen zu m√ºssen.
- **Chat-Verlauf**: Erinnert sich an Ihre vorherigen Gespr√§che (in einer Sitzung).
- **API**: LocalGPT verf√ºgt √ºber eine API, die Sie zum Erstellen von RAG-Anwendungen verwenden k√∂nnen.
- **Grafische Benutzeroberfl√§che**: LocalGPT wird mit zwei GUIs geliefert, eine verwendet die API und die andere ist eigenst√§ndig (basierend auf Streamlit).
- **GPU, CPU & MPS-Unterst√ºtzung**: Unterst√ºtzt mehrere Plattformen Out of the Box. Unterhalten Sie sich mit Ihren Daten unter Verwendung von `CUDA`, `CPU` oder `MPS` und mehr!

## Tauchen Sie tiefer ein mit unseren Videos üé•
- [Detaillierte Code-Durchsicht](https://youtu.be/MlyoObdIHyo)
- [Llama-2 mit LocalGPT](https://youtu.be/lbFmceo4D5E)
- [Chat-Verlauf hinzuf√ºgen](https://youtu.be/d7otIM_MCZs)
- [LocalGPT - Aktualisiert (17.09.2023)](https://youtu.be/G_prHSKX9d4)

## Technische Details üõ†Ô∏è
Durch die Auswahl der richtigen lokalen Modelle und die Kraft von `LangChain` k√∂nnen Sie die gesamte RAG-Pipeline lokal ausf√ºhren, ohne dass Daten Ihre Umgebung verlassen, und das mit akzeptabler Leistung.

- `ingest.py` verwendet `LangChain`-Werkzeuge, um das Dokument zu analysieren und Einbettungen lokal mit `InstructorEmbeddings` zu erstellen. Anschlie√üend wird das Ergebnis in einer lokalen Vektordatenbank mit `Chroma` Vektor-Speicher gespeichert.
- `run_localGPT.py` verwendet ein lokales LLM, um Fragen zu verstehen und Antworten zu erstellen. Der Kontext f√ºr die Antworten wird aus dem lokalen Vektor-Speicher extrahiert, indem eine √Ñhnlichkeitssuche durchgef√ºhrt wird, um den richtigen Kontextteil aus den Dokumenten zu finden.
- Sie k√∂nnen dieses lokale LLM durch jedes andere LLM von HuggingFace ersetzen. Stellen Sie sicher, dass das LLM, das Sie ausw√§hlen, im HF-Format ist.

Dieses Projekt wurde inspiriert vom Original [privateGPT](https://github.com/imartinez/privateGPT).

## Gebaut mit üß©
- [LangChain](https://github.com/hwchase17/langchain)
- [HuggingFace LLMs](https://huggingface.co/models)
- [InstructorEmbeddings](https://instructor-embedding.github.io/)
- [LLAMACPP](https://github.com/abetlen/llama-cpp-python)
- [ChromaDB](https://www.trychroma.com/)
- [Streamlit](https://streamlit.io/)

# Umgebungseinrichtung üåç

1. üì• Klone das Repo mit git:

```shell
git clone https://github.com/PromtEngineer/localGPT.git
```

2. üêç Installiere [conda](https://www.anaconda.com/download) f√ºr die Verwaltung virtueller Umgebungen. Erstelle und aktiviere eine neue virtuelle Umgebung.

```shell
conda create -n localGPT python=3.10.0
conda activate localGPT
```

3. üõ†Ô∏è Installiere die Abh√§ngigkeiten mit pip

Richten Sie Ihre Umgebung ein, um den Code auszuf√ºhren, und installieren Sie zun√§chst alle Anforderungen:

```shell
pip install -r requirements.txt
```

***LLAMA-CPP installieren:***

LocalGPT verwendet [LlamaCpp-Python](https://github.com/abetlen/llama-cpp-python) f√ºr GGML (Sie ben√∂tigen llama-cpp-python <=0.1.76) und GGUF (llama-cpp-python >=0.1.83) Modelle.

Falls Sie BLAS oder Metal mit [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal) verwenden m√∂chten, k√∂nnen Sie entsprechende Flags setzen:

F√ºr `NVIDIA` GPUs-Unterst√ºtzung verwenden Sie `cuBLAS`

```shell
# Beispiel: cuBLAS
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.83 --no-cache-dir
```

F√ºr Apple Metal (`M1/M2`) Unterst√ºtzung verwenden Sie

```shell
# Beispiel: METAL
CMAKE_ARGS="-DLLAMA_METAL=on"  FORCE_CMAKE=1 pip install llama-cpp-python==0.1.83 --no-cache-dir
```
Weitere Details finden Sie unter [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal).

## Docker üê≥

Das Installieren der ben√∂tigten Pakete f√ºr GPU-Inferenzen auf NVIDIA-GPUs, wie gcc 11 und CUDA 11, kann zu Konflikten mit anderen Paketen in Ihrem System f√ºhren.
Als Alternative zu Conda k√∂nnen Sie Docker mit der bereitgestellten Dockerfile verwenden.
Es beinhaltet CUDA, Ihr System ben√∂tigt nur Docker, BuildKit, Ihren NVIDIA GPU-Treiber und das NVIDIA Container Toolkit.
Bauen als `docker build . -t localgpt`, erfordert BuildKit.
Docker BuildKit unterst√ºtzt GPU w√§hrend der *docker build*-Zeit derzeit nicht, nur w√§hrend der *docker run*.
Ausf√ºhren als `docker run -it --mount src="$HOME/.cache",target=/root/.cache,type=bind --gpus=all localgpt`.

## Testdatensatz



Zum Testen wird dieses Repository mit der [Verfassung der USA](https://constitutioncenter.org/media/files/constitution.pdf) als Beispiel-Datei geliefert.

## Ihre EIGENEN Daten einpflegen.
Legen Sie Ihre Dateien in den Ordner `SOURCE_DOCUMENTS`. Sie k√∂nnen mehrere Ordner innerhalb des Ordners `SOURCE_DOCUMENTS` anlegen und der Code wird Ihre Dateien rekursiv lesen.

### Unterst√ºtzte Dateiformate:
LocalGPT unterst√ºtzt derzeit die folgenden Dateiformate. LocalGPT verwendet `LangChain` zum Laden dieser Dateiformate. Der Code in `constants.py` verwendet ein `DOCUMENT_MAP`-W√∂rterbuch, um ein Dateiformat dem entsprechenden Loader zuzuordnen. Um die Unterst√ºtzung f√ºr ein weiteres Dateiformat hinzuzuf√ºgen, f√ºgen Sie einfach dieses W√∂rterbuch mit dem Dateiformat und dem entsprechenden Loader aus [LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/) hinzu.

```shell
DOCUMENT_MAP = {
    ".txt": TextLoader,
    ".md": TextLoader,
    ".py": TextLoader,
    ".pdf": PDFMinerLoader,
    ".csv": CSVLoader,
    ".xls": UnstructuredExcelLoader,
    ".xlsx": UnstructuredExcelLoader,
    ".docx": Docx2txtLoader,
    ".doc": Docx2txtLoader,
}
```

### Ingest

F√ºhren Sie den folgenden Befehl aus, um alle Daten einzupflegen.

Wenn Sie `cuda` auf Ihrem System eingerichtet haben.

```shell
python ingest.py
```
Sie werden eine Ausgabe wie diese sehen:
<img width="1110" alt="Screenshot 2023-09-14 at 3 36 27 PM" src="https://github.com/PromtEngineer/localGPT/assets/134474669/c9274e9a-842c-49b9-8d95-606c3d80011f">


Verwenden Sie das Argument `device_type`, um einen bestimmten Ger√§tetyp anzugeben.
Zum Ausf√ºhren auf `cpu`

```sh
python ingest.py --device_type cpu
```

Zum Ausf√ºhren auf `M1/M2`

```sh
python ingest.py --device_type mps
```

Verwenden Sie `help` f√ºr eine vollst√§ndige Liste der unterst√ºtzten Ger√§te.

```sh
python ingest.py --help
```

Dies wird einen neuen Ordner namens `DB` erstellen und ihn f√ºr den neu erstellten Vektor-Speicher verwenden. Sie k√∂nnen so viele Dokumente einpflegen, wie Sie m√∂chten, und alle werden in der lokalen Einbettungsdatenbank angesammelt.
Wenn Sie von einer leeren Datenbank aus starten m√∂chten, l√∂schen Sie die `DB` und nehmen Sie die Einpflege Ihrer Dokumente erneut vor.

Hinweis: Wenn Sie dies zum ersten Mal ausf√ºhren, ben√∂tigen Sie Internetzugang, um das Einbettungsmodell herunterzuladen (Standard: `Instructor Embedding`). In den folgenden L√§ufen werden keine Daten Ihre lokale Umgebung verlassen und Sie k√∂nnen Daten ohne Internetverbindung einpflegen.

## Stellen Sie Ihren Dokumenten Fragen, lokal!

Um mit Ihren Dokumenten zu chatten, f√ºhren Sie den folgenden Befehl aus (standardm√§√üig wird es auf `cuda` ausgef√ºhrt).

```shell
python run_localGPT.py
```
Sie k√∂nnen auch den Ger√§tetyp angeben, genau wie `ingest.py`

```shell
python run_localGPT.py --device_type mps # zum Ausf√ºhren auf Apple Silicon
```

Dies wird den eingepflegten Vektor-Speicher und das Einbettungsmodell laden. Ihnen wird eine Aufforderung pr√§sentiert:

```shell
> Geben Sie eine Abfrage ein:
```

Nachdem Sie Ihre Frage eingegeben haben, dr√ºcken Sie die Eingabetaste. LocalGPT wird je nach Ihrer Hardware einige Zeit in Anspruch nehmen. Sie erhalten eine Antwort wie die untenstehende.
<img width="1312" alt="Screenshot 2023-09-14 at 3 33 19 PM" src="https://github.com/PromtEngineer/localGPT/assets/134474669/a7268de9-ade0-420b-a00b-ed12207dbe41">

Sobald die Antwort generiert wurde, k√∂nnen Sie eine weitere Frage stellen, ohne das Skript erneut auszuf√ºhren. Warten Sie einfach auf die Aufforderung.

***Hinweis:*** Wenn Sie dies zum ersten Mal ausf√ºhren, ben√∂tigen Sie eine Internetverbindung, um das LLM herunterzuladen (Standard: `TheBloke/Llama-2-7b-Chat-GGUF`). Danach k√∂nnen Sie Ihre Internetverbindung ausschalten und die Skriptinferenz w√ºrde immer noch funktionieren. Es gelangen keine Daten aus Ihrer lokalen Umgebung heraus.

Geben Sie `exit` ein, um das Skript zu beenden.

### Zus√§tzliche Optionen mit run_localGPT.py

Sie k√∂nnen die Flag `--show_sources` mit `run_localGPT.py` verwenden, um anzuzeigen, welche Bl√∂cke vom Einbettungsmodell abgerufen wurden. Standardm√§√üig werden 4 verschiedene Quellen/Bl√∂cke angezeigt. Sie k√∂nnen die Anzahl der Quellen/Bl√∂cke √§ndern

```shell
python run_localGPT.py --show_sources
```

Eine andere Option ist die Aktivierung des Chat-Verlaufs. ***Hinweis***: Dies ist standardm√§√üig deaktiviert und kann durch Verwendung der Flag `--use_history` aktiviert werden. Das Kontextfenster ist begrenzt, so dass das Aktivieren des Verlaufs es verwenden und m√∂glicherweise √ºberlaufen kann.

```shell
python run_localGPT.py --use_history
```

# Starten der grafischen Benutzeroberfl√§che

1. √ñffnen

 Sie `constants.py` in einem Editor Ihrer Wahl und f√ºgen Sie je nach Wahl das LLM hinzu, das Sie verwenden m√∂chten. Standardm√§√üig wird das folgende Modell verwendet:

   ```shell
   MODEL_ID = "TheBloke/Llama-2-7b-Chat-GGUF"
   MODEL_BASENAME = "llama-2-7b-chat.Q4_K_M.gguf"
   ```

3. √ñffnen Sie ein Terminal und aktivieren Sie Ihre Python-Umgebung, die die in requirements.txt installierten Abh√§ngigkeiten enth√§lt.

4. Navigieren Sie zum Verzeichnis `/LOCALGPT`.

5. F√ºhren Sie den folgenden Befehl aus: `python run_localGPT_API.py`. Die API sollte zu laufen beginnen.

6. Warten Sie, bis alles geladen ist. Sie sollten so etwas sehen wie `INFO:werkzeug:Press CTRL+C to quit`.

7. √ñffnen Sie ein zweites Terminal und aktivieren Sie dieselbe Python-Umgebung.

8. Navigieren Sie zum Verzeichnis `/LOCALGPT/localGPTUI`.

9. F√ºhren Sie den Befehl `python localGPTUI.py` aus.

10. √ñffnen Sie einen Webbrowser und gehen Sie zur Adresse `http://localhost:5111/`.


# Wie w√§hlt man unterschiedliche LLM-Modelle aus?

Um die Modelle zu √§ndern, m√ºssen Sie sowohl `MODEL_ID` als auch `MODEL_BASENAME` festlegen.

1. √ñffnen Sie `constants.py` in dem Editor Ihrer Wahl.
2. √Ñndern Sie `MODEL_ID` und `MODEL_BASENAME`. Wenn Sie ein quantisiertes Modell verwenden (`GGML`, `GPTQ`, `GGUF`), m√ºssen Sie `MODEL_BASENAME` angeben. Bei nicht quantisierten Modellen setzen Sie `MODEL_BASENAME` auf `NONE`
5. Es gibt eine Reihe von Beispielmodellen von HuggingFace, die bereits getestet wurden, um mit dem urspr√ºnglich trainierten Modell (Endung mit HF oder haben eine .bin in seinen "Dateien und Versionen") und quantisierten Modellen (Endung mit GPTQ oder haben eine .no-act-order oder .safetensors in seinen "Dateien und Versionen") ausgef√ºhrt zu werden.
6. F√ºr Modelle, die mit HF enden oder eine .bin in ihren "Dateien und Versionen" auf ihrer HuggingFace-Seite haben.

   - Stellen Sie sicher, dass Sie ein `MODEL_ID` ausgew√§hlt haben. Zum Beispiel -> `MODEL_ID = "TheBloke/guanaco-7B-HF"`
   - Gehen Sie zur [HuggingFace Repo](https://huggingface.co/TheBloke/guanaco-7B-HF)

7. F√ºr Modelle, die GPTQ in ihrem Namen enthalten und/oder eine .no-act-order oder .safetensors-Erweiterung in ihren "Dateien und Versionen" auf ihrer HuggingFace-Seite haben.

   - Stellen Sie sicher, dass Sie ein `MODEL_ID` ausgew√§hlt haben. Zum Beispiel -> model_id = `"TheBloke/wizardLM-7B-GPTQ"`
   - Gehen Sie zur entsprechenden [HuggingFace Repo](https://huggingface.co/TheBloke/wizardLM-7B-GPTQ) und w√§hlen Sie "Dateien und Versionen".
   - W√§hlen Sie einen der Modellnamen aus und setzen Sie ihn als `MODEL_BASENAME`. Zum Beispiel -> `MODEL_BASENAME = "wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors"`

8. Folgen Sie den gleichen Schritten f√ºr `GGUF` und `GGML`-Modelle.

# GPU- und VRAM-Anforderungen

Unten finden Sie die VRAM-Anforderungen f√ºr verschiedene Modelle, abh√§ngig von ihrer Gr√∂√üe (Milliarden von Parametern). Die Sch√§tzungen in der Tabelle beinhalten nicht den VRAM, der von den Einbettungsmodellen verwendet wird - diese verwenden zus√§tzlich 2 GB - 7 GB VRAM, abh√§ngig vom Modell.

| Modellgr√∂√üe (B) | float32   | float16   | GPTQ 8bit      | GPTQ 4bit          |
| ------- | --------- | --------- | -------------- | ------------------ |
| 7B      | 28 GB     | 14 GB     | 7 GB - 9 GB    | 3,5 GB - 5 GB      |
| 13B     | 52 GB     | 26 GB     | 13 GB - 15 GB  | 6,5 GB - 8 GB      |
| 32B     | 130 GB    | 65 GB     | 32,5 GB - 35 GB| 16,25 GB - 19 GB   |
| 65B     | 260,8 GB  | 130,4 GB  | 65,2 GB - 67 GB| 32,6 GB - 35 GB    |


# Systemanforderungen

## Python-Version

Um diese Software zu verwenden, m√ºssen Sie Python 3.10 oder eine neuere Version installiert haben. Fr√ºhere Versionen von Python werden nicht kompiliert.

## C++ Compiler

Wenn Sie w√§hrend des `pip install`-Prozesses einen Fehler beim Erstellen eines Rades erhalten, m√ºssen Sie m√∂glicherweise einen C++-Compiler auf Ihrem Computer installieren.

### F√ºr Windows 10/11

Um einen C++-Compiler unter Windows 10/11 zu installieren, gehen Sie wie folgt vor:

1. Installieren Sie Visual Studio 2022.
2. Stellen Sie sicher, dass die folgenden Komponenten ausgew√§hlt sind:
   - Universal Windows Platform development
   - C++ CMake tools f√ºr Windows
3. Laden Sie den MinGW-Installer von der [MinGW-Website](https://sourceforge.net/projects/mingw/) herunter.
4. F√ºhren Sie den Installer aus und w√§hlen Sie die Komponente "gcc" aus.

### NVIDIA-Treiberprobleme:

Folgen Sie dieser [Seite](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04), um NVIDIA-Treiber zu installieren.

## Sternenhistorie

[![Sternenhistorie-Diagramm](https://api.star-history.com/svg?repos=PromtEngineer/localGPT&type=Date)](https://star-history.com/#PromtEngineer/localGPT&Date)

# Haftungsausschluss

Dies ist ein Testprojekt zur √úberpr√ºfung der Machbarkeit einer vollst√§ndig lokalen L√∂sung f√ºr die Beantwortung von Fragen mit LLMs und Vektoreinbettungen. Es ist nicht produktionsbereit und nicht f√ºr den Einsatz in der Produktion vorgesehen. Vicuna-7B basiert auf dem Llama-Modell, sodass es die urspr√ºngliche Llama-Lizenz hat.

# H√§ufig
